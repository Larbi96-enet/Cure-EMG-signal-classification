{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROCESSEDDATA (another features combination) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJODZcHLh3g9P60zOxafTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Larbi96-enet/Cure-EMG-signal-classification/blob/main/PROCESSEDDATA_(another_features_combination)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aouS8yauxdmx"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "from tsfresh import extract_features, extract_relevant_features, select_features\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters, EfficientFCParameters\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc2kUiDcxkCL",
        "outputId": "c0639fb6-54e0-42fb-9880-03fb6eb3b936"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIHyxVsBxonq",
        "outputId": "76e268ae-b39d-49d3-b613-3e18aefadf3a"
      },
      "source": [
        "Processed_data = pd.read_csv('/content/drive/MyDrive/data/ProcessedEMG.csv',header = None)\n",
        "Processed_data.columns = [\"channel0\",\"channel1\",\"channel2\",\"channel3\",\"channel4\",\"channel5\",\"channel6\",\"channel7\"]\n",
        "Processed_data = Processed_data.drop([0], axis=0)\n",
        "dflist=[]\n",
        "dflist.append(Processed_data)\n",
        "data0= pd.read_csv('/content/drive/MyDrive/data/EMG-data.csv')\n",
        "classes = data0['class']\n",
        "labels=data0['label']\n",
        "dflist.append(classes)\n",
        "dflist.append(labels)\n",
        "df=pd.concat(dflist,axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,3,4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkwFdxgVyW8e",
        "outputId": "bc7aa5ee-4cd8-4253-cfd4-73032704893c"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "channel0     object\n",
              "channel1     object\n",
              "channel2     object\n",
              "channel3     object\n",
              "channel4     object\n",
              "channel5     object\n",
              "channel6     object\n",
              "channel7     object\n",
              "class       float64\n",
              "label       float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gSmvMhFyXde"
      },
      "source": [
        "df[\"channel0\"]=df.astype(float)\n",
        "df[\"channel1\"]=df.astype(float)\n",
        "df[\"channel2\"]=df.astype(float)\n",
        "df[\"channel3\"]=df.astype(float)\n",
        "df[\"channel4\"]=df.astype(float)\n",
        "df[\"channel5\"]=df.astype(float)\n",
        "df[\"channel6\"]=df.astype(float)\n",
        "df[\"channel7\"]=df.astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qSLmY8lydXQ",
        "outputId": "289b5386-cd62-4740-8ab7-586dd698b40c"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "channel0    float64\n",
              "channel1    float64\n",
              "channel2    float64\n",
              "channel3    float64\n",
              "channel4    float64\n",
              "channel5    float64\n",
              "channel6    float64\n",
              "channel7    float64\n",
              "class       float64\n",
              "label       float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "xIq9KoazyiYi",
        "outputId": "764895d0-34ac-4e5b-8e64-9b33b5063c21"
      },
      "source": [
        "df.drop([0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channel0</th>\n",
              "      <th>channel1</th>\n",
              "      <th>channel2</th>\n",
              "      <th>channel3</th>\n",
              "      <th>channel4</th>\n",
              "      <th>channel5</th>\n",
              "      <th>channel6</th>\n",
              "      <th>channel7</th>\n",
              "      <th>class</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>1.462582e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>3.448808e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>1.426897e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>1.260082e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>1.018470e-05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237903</th>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>8.297107e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237904</th>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>2.085376e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237905</th>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>7.471378e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237906</th>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>1.431597e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4237907</th>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>8.321392e-07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4237907 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             channel0      channel1      channel2  ...      channel7  class  label\n",
              "1        1.462582e-06  1.462582e-06  1.462582e-06  ...  1.462582e-06    0.0    1.0\n",
              "2        3.448808e-06  3.448808e-06  3.448808e-06  ...  3.448808e-06    0.0    1.0\n",
              "3        1.426897e-05  1.426897e-05  1.426897e-05  ...  1.426897e-05    0.0    1.0\n",
              "4        1.260082e-05  1.260082e-05  1.260082e-05  ...  1.260082e-05    0.0    1.0\n",
              "5        1.018470e-05  1.018470e-05  1.018470e-05  ...  1.018470e-05    0.0    1.0\n",
              "...               ...           ...           ...  ...           ...    ...    ...\n",
              "4237903  8.297107e-07  8.297107e-07  8.297107e-07  ...  8.297107e-07    0.0   36.0\n",
              "4237904  2.085376e-06  2.085376e-06  2.085376e-06  ...  2.085376e-06    0.0   36.0\n",
              "4237905  7.471378e-07  7.471378e-07  7.471378e-07  ...  7.471378e-07    0.0   36.0\n",
              "4237906  1.431597e-06  1.431597e-06  1.431597e-06  ...  1.431597e-06    0.0   36.0\n",
              "4237907  8.321392e-07  8.321392e-07  8.321392e-07  ...  8.321392e-07    NaN    NaN\n",
              "\n",
              "[4237907 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HjUghjCzwlQ",
        "outputId": "137e9cf8-9a96-4146-a618-95146ba611d4"
      },
      "source": [
        "subjects_data=df.groupby(['label','class'])\n",
        "print(type(subjects_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h96yIWYOyoan"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def rms(data): #root mean square\n",
        "      return  np.sqrt(np.mean(data**2,axis=0))  \n",
        "\n",
        "def abs_diffs_signal(data):\n",
        "    return np.sum(np.abs(np.diff(data,axis=0)),axis=0)\n",
        "\n",
        "\n",
        "def SD(data):\n",
        "    return np.std(data)\n",
        "\n",
        "def RMS (data):\n",
        "  ms = 0\n",
        "  for i in data:  \n",
        "    ms = ms + i*i\n",
        "    ms = ms / len(data)\n",
        "    RMS = np.sqrt(ms)\n",
        "    return RMS \n",
        "\n",
        "def MAV (data):\n",
        "    sum = 0\n",
        "    for i in data:\n",
        "        sum += np.absolute(i)\n",
        "    MAV = sum/len(data)\n",
        "    return MAV\n",
        "\n",
        "'''def WL (data):\n",
        "    sum = 0\n",
        "    for i in range(len(data)-1):\n",
        "        sum += np.absolute(data[i+1]-data[i])\n",
        "    WL = np.log(sum)\n",
        "    return WL '''\n",
        "def IEMG (data):\n",
        "     IEMG = np.trapz(data)\n",
        "     return IEMG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwxZ2Y3bzkr0",
        "outputId": "27c3bee4-d83f-4788-ddae-e4ea91d6baae"
      },
      "source": [
        "from time import time\n",
        "start=time()\n",
        "features=subjects_data.agg([ 'min','max',np.ptp,rms,RMS,abs_diffs_signal,MAV,IEMG])\n",
        "\n",
        "end=time()\n",
        "print('feature calculation time of one subject one class')\n",
        "print((end-start)/(36*7))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature calculation time of one subject one class\n",
            "0.16689475377400717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "QFCHf5iPztMQ",
        "outputId": "bfec0561-541a-4ede-83cd-6d0621a6a28d"
      },
      "source": [
        "features=features.reset_index()\n",
        "features.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>class</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel0</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel1</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel2</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel3</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel4</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel5</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel6</th>\n",
              "      <th colspan=\"8\" halign=\"left\">channel7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>ptp</th>\n",
              "      <th>rms</th>\n",
              "      <th>RMS</th>\n",
              "      <th>abs_diffs_signal</th>\n",
              "      <th>MAV</th>\n",
              "      <th>IEMG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.873120e-10</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>8.374821e-10</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>4.540891e-08</td>\n",
              "      <td>0.014032</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.047628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "      <td>6.486447e-09</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>5.972687e-07</td>\n",
              "      <td>0.215433</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.607589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "      <td>1.085570e-08</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>2.135893e-07</td>\n",
              "      <td>0.184099</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.589099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "      <td>2.015412e-09</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>8.103952e-07</td>\n",
              "      <td>0.045062</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.137404</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label class      channel0  ...         channel7                    \n",
              "                        min  ... abs_diffs_signal       MAV      IEMG\n",
              "0   1.0   0.0  1.873120e-10  ...              NaN       NaN       NaN\n",
              "1   1.0   1.0  8.374821e-10  ...         0.014032  0.000007  0.047628\n",
              "2   1.0   2.0  6.486447e-09  ...         0.215433  0.000089  0.607589\n",
              "3   1.0   3.0  1.085570e-08  ...         0.184099  0.000080  0.589099\n",
              "4   1.0   4.0  2.015412e-09  ...         0.045062  0.000020  0.137404\n",
              "\n",
              "[5 rows x 66 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EzSw0F20ltn",
        "outputId": "ed99f56e-bbd8-4ee0-a1a8-748b5004f899"
      },
      "source": [
        "features=features.dropna()\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(253, 66)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfCwUZDL0pU0"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report,accuracy_score,recall_score,roc_auc_score, precision_score,f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold,StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfHSVsLF0s0N"
      },
      "source": [
        "classifiers = [#manhattan,4,distance\n",
        "    KNeighborsClassifier(metric='manhattan',weights='distance',n_neighbors=4),\n",
        "    SVC(C=3, gamma=0.04), #2,0.01\n",
        "    DecisionTreeClassifier(criterion= 'entropy',max_depth= 7,max_features= 'auto',random_state= 20),\n",
        "    RandomForestClassifier(bootstrap=False,max_depth=8,max_features='sqrt',min_samples_leaf=1,\n",
        "                          min_samples_split=2,n_estimators=100),\n",
        "    GradientBoostingClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    LogisticRegression(C=0.05, penalty='l2')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cgorc_y0vnn",
        "outputId": "1d74300a-60e2-4442-f29a-1cb61266f8ec"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('always') \n",
        "clf_performace={'accuracy':[],'f1score':[],'training_time':[],'testing_time':[]}\n",
        "for clfs in classifiers:\n",
        "    #pipeline.set_params(estimator = clfs)\n",
        "    name = clfs.__class__.__name__\n",
        "    accuracy=[]\n",
        "    f1score=[]\n",
        "    recall=[]\n",
        "    precision=[]\n",
        "    training_time=[]\n",
        "    testing_time=[]\n",
        "    for i in range(1,37):\n",
        "        train=features[features['label']!=i]\n",
        "        test=features[features['label']==i]\n",
        "        X_train=train.iloc[:,2::].values\n",
        "        X_test=test.iloc[:,2::].values\n",
        "        y_train=train['class']\n",
        "        y_test=test['class']\n",
        "        training_start=time()\n",
        "        clf = make_pipeline(StandardScaler(), clfs)\n",
        "        clf.fit(X_train, y_train)\n",
        "        training_end=time()\n",
        "        testing_start=time()\n",
        "        y_pred=clf.predict(X_test)\n",
        "        testing_end=time()\n",
        "        acc=accuracy_score(y_test,y_pred)\n",
        "        recall.append(recall_score(y_test,y_pred,average='macro'))\n",
        "        #precision.append(precision_score(y_test,y_pred,average='macro'))\n",
        "        f1score.append(f1_score(y_test,y_pred,average='macro'))\n",
        "        accuracy.append(acc)\n",
        "        training_time.append(training_end-training_start)\n",
        "        testing_time.append(testing_end-testing_start)\n",
        "        #print('subject {} accuracy is {}'.format(i,acc))\n",
        "    print(name,np.mean(accuracy))\n",
        "    clf_performace['accuracy'].append(accuracy)\n",
        "    clf_performace['f1score'].append(f1score)\n",
        "    clf_performace['training_time'].append(np.mean(training_time))\n",
        "    clf_performace['testing_time'].append(np.mean(testing_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier 0.49437830687830686\n",
            "SVC 0.5023148148148148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DecisionTreeClassifier 0.4859457671957672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier 0.4624669312169313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GradientBoostingClassifier 0.48296957671957674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GaussianNB 0.5054563492063492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LinearDiscriminantAnalysis 0.462797619047619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.5178571428571429\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "e4-hQ0_d02Qj",
        "outputId": "601d0c22-c7e3-4cb2-c634-9ed4da5bb439"
      },
      "source": [
        "df=pd.DataFrame(np.stack((a,f)),columns=['KNN','SVM','DT','RF','GB','NB','LDA','LR'],\n",
        "            index=['Accuracy','F1-score'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>KNN</th>\n",
              "      <th>SVM</th>\n",
              "      <th>DT</th>\n",
              "      <th>RF</th>\n",
              "      <th>GB</th>\n",
              "      <th>NB</th>\n",
              "      <th>LDA</th>\n",
              "      <th>LR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.486938</td>\n",
              "      <td>0.494378</td>\n",
              "      <td>0.470734</td>\n",
              "      <td>0.498181</td>\n",
              "      <td>0.482970</td>\n",
              "      <td>0.500827</td>\n",
              "      <td>0.478175</td>\n",
              "      <td>0.517857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F1-score</th>\n",
              "      <td>0.440454</td>\n",
              "      <td>0.437831</td>\n",
              "      <td>0.426135</td>\n",
              "      <td>0.443981</td>\n",
              "      <td>0.436111</td>\n",
              "      <td>0.424184</td>\n",
              "      <td>0.402756</td>\n",
              "      <td>0.445122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               KNN       SVM        DT  ...        NB       LDA        LR\n",
              "Accuracy  0.486938  0.494378  0.470734  ...  0.500827  0.478175  0.517857\n",
              "F1-score  0.440454  0.437831  0.426135  ...  0.424184  0.402756  0.445122\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "tNFtdvvO05Al",
        "outputId": "e5619073-794b-493b-8781-2f182db14352"
      },
      "source": [
        "classifier=df.loc['F1-score'].index\n",
        "y_pos = np.arange(len(classifier))\n",
        "w=0.4\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(y_pos,df.loc['Accuracy'].values,align='center',width=w,label='Accuracy',color='black')\n",
        "plt.bar(y_pos+w,df.loc['F1-score'].values,align='center',width=w,label='F1-score',color='blue')\n",
        "\n",
        "ya=df.loc['Accuracy'].values\n",
        "for index, value in enumerate(ya):\n",
        "    plt.text(index-0.1,value-0.2, str(np.round(value,2)),rotation=90,color='white',fontsize=12)\n",
        "\n",
        "yf=df.loc['F1-score'].values\n",
        "for index, value in enumerate(yf):\n",
        "    plt.text(index+w-0.1,value-0.2, str(np.round(value,2)),rotation=90,color='white',fontsize=12)\n",
        "\n",
        "\n",
        "plt.xticks(y_pos, classifier,fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "plt.ylabel('Score',fontsize=12)\n",
        "plt.xlabel('Classifiers',fontsize=12)\n",
        "plt.title('Classifiers Performance',fontsize=12)\n",
        "plt.legend()\n",
        "plt.savefig('result.eps',dip=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
            "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "M7EvxQ5l08Gd",
        "outputId": "5be0135d-9bc5-4d17-f919-48a23f61105e"
      },
      "source": [
        "pd.DataFrame(zip(np.array(clf_performace['training_time'])*65,np.array(clf_performace['testing_time'])),\n",
        "            columns=['training_time','testing_time'],\n",
        "            index=['KNN','SVM','DT','RF','GB','NB','LDA','LR'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>training_time</th>\n",
              "      <th>testing_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.150587</td>\n",
              "      <td>0.001561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.711512</td>\n",
              "      <td>0.000550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DT</th>\n",
              "      <td>0.410531</td>\n",
              "      <td>0.000267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RF</th>\n",
              "      <td>13.882049</td>\n",
              "      <td>0.008617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GB</th>\n",
              "      <td>179.828235</td>\n",
              "      <td>0.001120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NB</th>\n",
              "      <td>0.132135</td>\n",
              "      <td>0.000490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LDA</th>\n",
              "      <td>0.504833</td>\n",
              "      <td>0.000367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR</th>\n",
              "      <td>3.754622</td>\n",
              "      <td>0.000320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     training_time  testing_time\n",
              "KNN       0.150587      0.001561\n",
              "SVM       0.711512      0.000550\n",
              "DT        0.410531      0.000267\n",
              "RF       13.882049      0.008617\n",
              "GB      179.828235      0.001120\n",
              "NB        0.132135      0.000490\n",
              "LDA       0.504833      0.000367\n",
              "LR        3.754622      0.000320"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz6MUusc0_1d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}